{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diz24xtpMMmz",
        "outputId": "ab07e6a1-f221-4004-eaa9-f3751282979d"
      },
      "outputs": [],
      "source": [
        "# ! git clone https://github.com/huggingface/diffusers && cd diffusers && pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjVmq9qJMST4",
        "outputId": "b02dc7c9-1fc7-46ee-91af-3c49836538f6"
      },
      "outputs": [],
      "source": [
        "# ! cd diffusers/examples/text_to_image  && pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0jBBAEHWOKUL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuG6r7YEOHM8",
        "outputId": "ebb50a0d-a53d-486d-8778-45196a6b98f0"
      },
      "outputs": [],
      "source": [
        "# !  git config --global credential.helper store && huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdUD57tMO140",
        "outputId": "8247373d-faee-42dc-bccb-6fba1f152fbf"
      },
      "outputs": [],
      "source": [
        "# ! accelerate launch --mixed_precision=\"fp16\"  train_text_to_image.py \\\n",
        "#   --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-2-1\" \\\n",
        "#   --dataset_name=\"mintz1104/diffusion_stage_design_japanese_anime_style\" \\\n",
        "#   --use_ema \\\n",
        "#   --resolution=768 --center_crop --random_flip \\\n",
        "#   --train_batch_size=1 \\\n",
        "#   --gradient_accumulation_steps=4 \\\n",
        "#   --gradient_checkpointing \\\n",
        "#   --max_train_steps=15000 \\\n",
        "#   --learning_rate=1e-05 \\\n",
        "#   --max_grad_norm=1 \\\n",
        "#   --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n",
        "#   --output_dir=\"tuned-model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# from datasets import Dataset, DatasetDict, Features, Value, Image\n",
        "\n",
        "\n",
        "# def load_drones_dataset(image_folder: str) -> DatasetDict:\n",
        "#     \"\"\"\n",
        "#     Загружает изображения и возвращает DatasetDict с набором train.\n",
        "\n",
        "#     Args:\n",
        "#         image_folder (str): Путь к папке с изображениями.\n",
        "\n",
        "#     Returns:\n",
        "#         DatasetDict: Датасет с разделением на train.\n",
        "#     \"\"\"\n",
        "#     image_files = [f for f in os.listdir(image_folder) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "#     data = {\n",
        "#         \"image\": [],\n",
        "#         \"text\": []\n",
        "#     }\n",
        "\n",
        "#     for idx, image_file in enumerate(image_files):\n",
        "#         image_path = os.path.join(image_folder, image_file)\n",
        "\n",
        "#         # Генерируем подпись как имя файла без расширения\n",
        "#         caption = os.path.splitext(image_file)[0]\n",
        "\n",
        "#         data[\"image\"].append(image_path)\n",
        "#         data[\"text\"].append(caption)\n",
        "\n",
        "#     # Определяем структуру данных (features) для корректного хранения\n",
        "#     features = Features({\n",
        "#         \"image\": Image(),  # Автоматическая загрузка изображений\n",
        "#         \"text\": Value(\"string\")  # Строка\n",
        "#     })\n",
        "\n",
        "#     # Создаем Dataset\n",
        "#     dataset = Dataset.from_dict(data, features=features)\n",
        "\n",
        "#     # Оборачиваем в DatasetDict с подмножеством train\n",
        "#     dataset_dict = DatasetDict({\"train\": dataset})\n",
        "\n",
        "#     return dataset_dict\n",
        "\n",
        "\n",
        "# def save_dataset(dataset: DatasetDict, output_folder: str):\n",
        "#     \"\"\"\n",
        "#     Сохраняет DatasetDict в указанную папку.\n",
        "\n",
        "#     Args:\n",
        "#         dataset (DatasetDict): Датасет, который нужно сохранить.\n",
        "#         output_folder (str): Путь к папке для сохранения.\n",
        "#     \"\"\"\n",
        "#     os.makedirs(output_folder, exist_ok=True)  # Создаем папку, если её нет\n",
        "#     dataset.save_to_disk(output_folder)\n",
        "#     print(f\"Датасет сохранён в формате DatasetDict в папку: {output_folder}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from datasets import load_from_disk\n",
        "\n",
        "# image_folder = \"./drone-dataset\"  \n",
        "# output_folder = \"./dataset\"  \n",
        "\n",
        "# dataset = load_drones_dataset(image_folder)\n",
        "# save_dataset(dataset, output_folder)\n",
        "\n",
        "# dataset_path = \"./dataset\"\n",
        "# dataset = load_from_disk(dataset_path)\n",
        "# print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# # Загрузка датасета\n",
        "# dataset = load_dataset(\"mintz1104/diffusion_stage_design_japanese_anime_style\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    \"drone lightshow of a glowing tree of life with intricate roots and branches, illuminated in green and gold, made with 2000 drones on a black background\",\n",
        "    \"drone lightshow of a detailed golden phoenix rising with glowing wings and fire-like trails made with 2000 drones on a black background\",\n",
        "    \"drone lightshow of the face of Elon Musk with intricate details made with 2000 drones on a black background\",\n",
        "    \"drone lightshow of a red and white heart-shaped pattern over the ocean with reflections of the lights made with 2000 drones\",\n",
        "    \"drone lightshow of a golden sun rising behind snow-capped mountains with glowing rays of light made with 2000 drones\",\n",
        "    \"drone lightshow of a detailed red and silver motorcycle speeding forward with glowing exhaust made with 1500 drones on a black background\",\n",
        "    \"drone lightshow of a bright multicolored butterfly flapping its wings over a field with trees and ambient lighting effects made with 2000 drones on a black background\",\n",
        "    \"drone lightshow of a shimmering blue and white globe of the Earth with detailed continents made with 1000 drones on a black background\",\n",
        "    \"drone lightshow of a futuristic robot standing tall with glowing blue eyes and moving arms made with 2000 drones on a black background\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stable-Diffusion 2.1: Fine tuning "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! accelerate launch --mixed_precision=\"fp16\"  diffusers/examples/text_to_image/train_text_to_image.py \\\n",
        "#   --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-2-1\" \\\n",
        "#   --train_data_dir=\"./dataset/\" \\\n",
        "#   --resolution=768 \\\n",
        "#   --center_crop \\\n",
        "#   --train_batch_size=2 \\\n",
        "#   --gradient_accumulation_steps=4 \\\n",
        "#   --gradient_checkpointing \\\n",
        "#   --max_train_steps=15000 \\\n",
        "#   --learning_rate=1e-05 \\\n",
        "#   --max_grad_norm=1 \\\n",
        "#   --lr_scheduler=\"cosine\" --lr_warmup_steps=100 \\\n",
        "#   --output_dir=\"tuned-model-2.1\" \\\n",
        "#   --checkpointing_steps=5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from diffusers import StableDiffusionPipeline, UNet2DConditionModel\n",
        "# import torch\n",
        "\n",
        "# # # Загрузить модель\n",
        "# # model_path = \"tuned-model-2.1\"\n",
        "# # pipe_tuned = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16).to(\"cuda\")\n",
        "\n",
        "# # Загрузить чекпоинт\n",
        "# model_path = \"stabilityai/stable-diffusion-2-1\"\n",
        "# pipe_tuned = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16).to(\"cuda\")\n",
        "# unet = UNet2DConditionModel.from_pretrained(\"tuned-model-2.1/checkpoint-15000\", subfolder=\"unet\", torch_dtype=torch.float16)\n",
        "# pipe_tuned.unet = unet\n",
        "# pipe_tuned.to(\"cuda\")\n",
        "\n",
        "\n",
        "# from diffusers import EulerAncestralDiscreteScheduler\n",
        "# pipe_tuned.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe_tuned.scheduler.config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from PIL import Image\n",
        "\n",
        "# # Параметры отображения\n",
        "# desired_height = 768\n",
        "# desired_width = 768\n",
        "\n",
        "# # Устанавливаем количество изображений в строке\n",
        "# images_per_row = 3\n",
        "\n",
        "# # Создаём фигуру для сетки\n",
        "# fig, axes = plt.subplots(\n",
        "#     nrows=(len(prompts) + images_per_row - 1) // images_per_row,  # Расчёт строк\n",
        "#     ncols=images_per_row,\n",
        "#     figsize=(15, 15)\n",
        "# )\n",
        "\n",
        "# # Генерация изображений\n",
        "# for i, prompt in enumerate(prompts):\n",
        "#     ax = axes[i // images_per_row, i % images_per_row]  # Выбираем позицию\n",
        "#     ax.axis(\"off\")  # Убираем оси\n",
        "#     ax.set_title(f\"Prompt {i + 1}\")\n",
        "\n",
        "#     # Генерация изображения\n",
        "#     tuned_image = pipe_tuned(\n",
        "#         prompt=prompt,\n",
        "#         num_inference_steps=70,\n",
        "#         guidance_scale=15,\n",
        "#         random_state=42,\n",
        "#         height=desired_height,\n",
        "#         width=desired_width\n",
        "#     ).images[0]\n",
        "\n",
        "#     # Уменьшаем размер для отображения\n",
        "#     small_image = tuned_image.resize((256, 256), Image.Resampling.LANCZOS)\n",
        "#     ax.imshow(small_image)\n",
        "\n",
        "# # Убираем лишние оси, если количество изображений не кратно `images_per_row`\n",
        "# for ax in axes.flat[len(prompts):]:\n",
        "#     ax.axis(\"off\")\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prompts = [\n",
        "#     \"drone lightshow of a glowing tree of life with intricate roots and branches, illuminated in green and gold, made with 2000 drones on a black background\",\n",
        "#     \"drone lightshow of a detailed golden phoenix rising with glowing wings and fire-like trails made with 2000 drones on a black background\",\n",
        "#     \"drone lightshow of the face of Elon Musk with intricate details made with 2000 drones on a black background\",\n",
        "#     \"drone lightshow of a red and white heart-shaped pattern over the ocean with reflections of the lights made with 2000 drones\",\n",
        "#     \"drone lightshow of a golden sun rising behind snow-capped mountains with glowing rays of light made with 2000 drones\",\n",
        "#     \"drone lightshow of a detailed red and silver motorcycle speeding forward with glowing exhaust made with 1500 drones on a black background\",\n",
        "#     \"drone lightshow of a bright multicolored butterfly flapping its wings over a field with trees and ambient lighting effects made with 2000 drones on a black background\",\n",
        "#     \"drone lightshow of a shimmering blue and white globe of the Earth with detailed continents made with 1000 drones on a black background\",\n",
        "#     \"drone lightshow of a futuristic robot standing tall with glowing blue eyes and moving arms made with 2000 drones on a black background\"\n",
        "# ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stable-Diffusion 2.1: Lora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! accelerate launch --mixed_precision=\"no\" diffusers/examples/text_to_image/train_text_to_image_lora.py \\\n",
        "#   --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-2-1\" \\\n",
        "#   --train_data_dir=\"./dataset/\" \\\n",
        "#   --caption_column=\"text\" \\\n",
        "#   --resolution=768 --random_flip \\\n",
        "#   --train_batch_size=1 \\\n",
        "#   --num_train_epochs=50 --checkpointing_steps=5000 \\\n",
        "#   --learning_rate=1e-04 \\\n",
        "#   --lr_scheduler=\"constant\" \\\n",
        "#   --lr_warmup_steps=0 \\\n",
        "#   --seed=42 \\\n",
        "#   --output_dir=\"tuned-model-lora\" \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from diffusers import StableDiffusionPipeline\n",
        "# import torch\n",
        "\n",
        "# model_path = \"tuned-model-2.1-lora\"\n",
        "# model_name = \"stabilityai/stable-diffusion-2-1\"\n",
        "# pipe_tuned = StableDiffusionPipeline.from_pretrained(model_name, torch_dtype=torch.float16)\n",
        "# pipe_tuned.unet.load_attn_procs(model_path)\n",
        "# pipe_tuned.to(\"cuda\")\n",
        "\n",
        "\n",
        "# from diffusers import EulerAncestralDiscreteScheduler\n",
        "# pipe_tuned.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe_tuned.scheduler.config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from PIL import Image\n",
        "\n",
        "# # Параметры отображения\n",
        "# desired_height = 768\n",
        "# desired_width = 768\n",
        "\n",
        "# # Устанавливаем количество изображений в строке\n",
        "# images_per_row = 3\n",
        "\n",
        "# # Создаём фигуру для сетки\n",
        "# fig, axes = plt.subplots(\n",
        "#     nrows=(len(prompts) + images_per_row - 1) // images_per_row,  # Расчёт строк\n",
        "#     ncols=images_per_row,\n",
        "#     figsize=(15, 15)\n",
        "# )\n",
        "\n",
        "# # Генерация изображений\n",
        "# for i, prompt in enumerate(prompts):\n",
        "#     ax = axes[i // images_per_row, i % images_per_row]  # Выбираем позицию\n",
        "#     ax.axis(\"off\")  # Убираем оси\n",
        "#     ax.set_title(f\"Prompt {i + 1}\")\n",
        "\n",
        "#     # Генерация изображения\n",
        "#     tuned_image = pipe_tuned(\n",
        "#         prompt=prompt,\n",
        "#         num_inference_steps=70,\n",
        "#         guidance_scale=6.5,\n",
        "#         random_state=42,\n",
        "#         height=desired_height,\n",
        "#         width=desired_width\n",
        "#     ).images[0]\n",
        "\n",
        "#     # Уменьшаем размер для отображения\n",
        "#     small_image = tuned_image.resize((256, 256), Image.Resampling.LANCZOS)\n",
        "#     ax.imshow(small_image)\n",
        "\n",
        "# # Убираем лишние оси, если количество изображений не кратно `images_per_row`\n",
        "# for ax in axes.flat[len(prompts):]:\n",
        "#     ax.axis(\"off\")\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prompts = [\n",
        "#     \"drone lightshow of a glowing tree of life with intricate roots and branches, illuminated in green and gold, made with 2000 drones on a black background\",\n",
        "#     \"drone lightshow of a detailed golden phoenix rising with glowing wings and fire-like trails made with 2000 drones on a black background\",\n",
        "#     \"drone lightshow of face of Elon Musk on a black background\",\n",
        "#     \"drone lightshow of a red and white heart-shaped pattern over the ocean with reflections of the lights made with 2000 drones\",\n",
        "#     \"drone lightshow of a golden sun rising behind snow-capped mountains with glowing rays of light made with 2000 drones\",\n",
        "#     \"drone lightshow of a detailed red and silver motorcycle speeding forward with glowing exhaust made with 1500 drones on a black background\",\n",
        "#     \"drone lightshow of a bright multicolored butterfly flapping its wings over a field with trees and ambient lighting effects made with 2000 drones on a black background\",\n",
        "#     \"drone lightshow of a shimmering blue and white globe of the Earth with detailed continents made with 1000 drones on a black background\",\n",
        "#     \"drone lightshow of a futuristic robot standing tall with glowing blue eyes and moving arms made with 2000 drones on a black background\"\n",
        "# ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stable diffusion 3.x: Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -r diffusers/examples/dreambooth/requirements_sd3.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! accelerate launch diffusers/examples/dreambooth/train_dreambooth_sd3.py \\\n",
        "  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-3.5-medium\"  \\\n",
        "  --dataset_name=\"./dataset/\" \\\n",
        "  --output_dir=\"tuned-model-3.5\" \\\n",
        "  --resolution=768 \\\n",
        "  --train_batch_size=2 \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --learning_rate=5e-6 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --image_column=\"image\" \\\n",
        "  --caption_column=\"text\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --gradient_checkpointing \\\n",
        "  --max_train_steps=6500 \\\n",
        "  --checkpointing_steps=5000 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # \"stabilityai/stable-diffusion-3.5-large\"\n",
        "# # \"stabilityai/stable-diffusion-3.5-medium\"\n",
        "# ! accelerate launch diffusers/examples/dreambooth/train_dreambooth_lora_sd3.py \\\n",
        "#   --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-3.5-medium\"  \\\n",
        "#   --dataset_name=\"./dataset/\" \\\n",
        "#   --output_dir=\"tuned-model-3.5-medium-lora\" \\\n",
        "#   --resolution=512 \\\n",
        "#   --train_batch_size=2 \\\n",
        "#   --gradient_accumulation_steps=1 \\\n",
        "#   --learning_rate=1e-4 \\\n",
        "#   --lr_scheduler=\"constant\" \\\n",
        "#   --lr_warmup_steps=0 \\\n",
        "#   --seed=\"0\" \\\n",
        "#   --image_column=\"image\" \\\n",
        "#   --caption_column=\"text\" \\\n",
        "#   --mixed_precision=\"fp16\" \\\n",
        "#   --use_8bit_adam \\\n",
        "#   --max_train_steps=40000 \\\n",
        "#   --checkpointing_steps=5000 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusion3Pipeline\n",
        "import torch\n",
        "\n",
        "model_path = \"tuned-model-3.5-medium-lora\"\n",
        "model_name = \"stabilityai/stable-diffusion-3.5-medium\"\n",
        "\n",
        "\n",
        "pipe_tuned = StableDiffusion3Pipeline.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cuda\")\n",
        "pipe_tuned.load_lora_weights(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "desired_height = 768\n",
        "desired_width = 768\n",
        "images_per_row = 3\n",
        "\n",
        "fig, axes = plt.subplots(\n",
        "    nrows=(len(prompts) + images_per_row - 1) // images_per_row,  # Расчёт строк\n",
        "    ncols=images_per_row,\n",
        "    figsize=(15, 15)\n",
        ")\n",
        "\n",
        "for i, prompt in enumerate(prompts):\n",
        "    ax = axes[i // images_per_row, i % images_per_row]  # Выбираем позицию\n",
        "    ax.axis(\"off\")  # Убираем оси\n",
        "    ax.set_title(f\"Prompt {i + 1}\")\n",
        "\n",
        "    tuned_image = pipe_tuned(\n",
        "        prompt=prompt,\n",
        "        num_inference_steps=100,\n",
        "        guidance_scale=6.5,\n",
        "        height=desired_height,\n",
        "        width=desired_width\n",
        "    ).images[0]\n",
        "\n",
        "    small_image = tuned_image.resize((256, 256), Image.Resampling.LANCZOS)\n",
        "    ax.imshow(small_image)\n",
        "\n",
        "for ax in axes.flat[len(prompts):]:\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! accelerate launch diffusers/examples/dreambooth/train_dreambooth_lora_sd3.py \\\n",
        "#   --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-3.5-large\"  \\\n",
        "#   --dataset_name=\"./dataset/\" \\\n",
        "#   --output_dir=\"tuned-model-3.5-large-lora\" \\\n",
        "#   --resolution=256 \\\n",
        "#   --train_batch_size=1 \\\n",
        "#   --gradient_accumulation_steps=1 \\\n",
        "#   --learning_rate=1e-4 \\\n",
        "#   --lr_scheduler=\"cosine\" \\\n",
        "#   --lr_warmup_steps=100 \\\n",
        "#   --seed=\"0\" \\\n",
        "#   --image_column=\"image\" \\\n",
        "#   --caption_column=\"text\" \\\n",
        "#   --mixed_precision=\"fp16\" \\\n",
        "#   --use_8bit_adam \\\n",
        "#   --max_train_steps=20000 \\\n",
        "#   --checkpointing_steps=5000 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from diffusers import StableDiffusion3Pipeline\n",
        "# import torch\n",
        "\n",
        "\n",
        "# model_path = \"tuned-model-3.5-large-lora\"\n",
        "# model_name = \"stabilityai/stable-diffusion-3.5-large\"\n",
        "\n",
        "# pipe_tuned = StableDiffusion3Pipeline.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cuda\")\n",
        "# pipe_tuned.load_lora_weights(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from PIL import Image\n",
        "\n",
        "# desired_height = 768\n",
        "# desired_width = 768\n",
        "# images_per_row = 3\n",
        "\n",
        "# fig, axes = plt.subplots(\n",
        "#     nrows=(len(prompts) + images_per_row - 1) // images_per_row,  # Расчёт строк\n",
        "#     ncols=images_per_row,\n",
        "#     figsize=(15, 15)\n",
        "# )\n",
        "\n",
        "# for i, prompt in enumerate(prompts):\n",
        "#     ax = axes[i // images_per_row, i % images_per_row]  # Выбираем позицию\n",
        "#     ax.axis(\"off\")  # Убираем оси\n",
        "#     ax.set_title(f\"Prompt {i + 1}\")\n",
        "\n",
        "#     tuned_image = pipe_tuned(\n",
        "#         prompt=prompt,\n",
        "#         num_inference_steps=100,\n",
        "#         guidance_scale=6.5,\n",
        "#         height=desired_height,\n",
        "#         width=desired_width\n",
        "#     ).images[0]\n",
        "\n",
        "#     small_image = tuned_image.resize((256, 256), Image.Resampling.LANCZOS)\n",
        "#     ax.imshow(small_image)\n",
        "\n",
        "# for ax in axes.flat[len(prompts):]:\n",
        "#     ax.axis(\"off\")\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Flux 1-dev: Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -r diffusers/examples/flux-control/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "accelerate launch  diffusers/examples/flux-control/train_control_flux.py \\\n",
        "  --pretrained_model_name_or_path=\"black-forest-labs/FLUX.1-dev\" \\\n",
        "  --dataset_name=\"raulc0399/open_pose_controlnet\" \\\n",
        "  --output_dir=\"tuned-model-flux\" \\\n",
        "  --mixed_precision=\"f16\" \\\n",
        "  --train_batch_size=2 \\\n",
        "  --dataloader_num_workers=4 \\\n",
        "  --gradient_accumulation_steps=4 \\\n",
        "  --gradient_checkpointing \\\n",
        "  --proportion_empty_prompts=0.2 \\\n",
        "  --learning_rate=5e-5 \\\n",
        "  --adam_weight_decay=1e-4 \\\n",
        "  --lr_scheduler=\"cosine\" \\\n",
        "  --lr_warmup_steps=1000 \\\n",
        "  --checkpointing_steps=1000 \\\n",
        "  --max_train_steps=10000 \\\n",
        "  --offload \\\n",
        "  --seed=\"0\" \\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from diffusers import FluxControlPipeline, FluxTransformer2DModel\n",
        "# import torch\n",
        "\n",
        "# # Инициализация модели FLUX\n",
        "# # transformer = FluxTransformer2DModel.from_pretrained(\"tuned-model-flux\")  \n",
        "# transformer = FluxTransformer2DModel.from_pretrained(\"black-forest-labs/FLUX.1-dev\") \n",
        "# pipe_tuned = FluxControlPipeline.from_pretrained(\n",
        "#     \"black-forest-labs/FLUX.1-dev\", \n",
        "#     transformer=transformer, \n",
        "#     torch_dtype=torch.float16\n",
        "# ).to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from PIL import Image\n",
        "\n",
        "# desired_height = 768\n",
        "# desired_width = 768\n",
        "# images_per_row = 3\n",
        "\n",
        "# fig, axes = plt.subplots(\n",
        "#     nrows=(len(prompts) + images_per_row - 1) // images_per_row,  # Расчёт строк\n",
        "#     ncols=images_per_row,\n",
        "#     figsize=(15, 15)\n",
        "# )\n",
        "\n",
        "# for i, prompt in enumerate(prompts):\n",
        "#     ax = axes[i // images_per_row, i % images_per_row]  # Выбираем позицию\n",
        "#     ax.axis(\"off\")  # Убираем оси\n",
        "#     ax.set_title(f\"Prompt {i + 1}\")\n",
        "\n",
        "#     tuned_image = pipe_tuned(\n",
        "#         prompt=prompt,\n",
        "#         num_inference_steps=70,\n",
        "#         guidance_scale=6.5,\n",
        "#         random_state=42,\n",
        "#         height=desired_height,\n",
        "#         width=desired_width\n",
        "#     ).images[0]\n",
        "\n",
        "#     small_image = tuned_image.resize((256, 256), Image.Resampling.LANCZOS)\n",
        "#     ax.imshow(small_image)\n",
        "\n",
        "# for ax in axes.flat[len(prompts):]:\n",
        "#     ax.axis(\"off\")\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv_35",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
